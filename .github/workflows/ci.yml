name: GPU Benchmark CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch: # Allow manual trigger

permissions:
  contents: read     # Default read access
  pages: write       # To deploy the report via GitHub Pages
  id-token: write    # To authenticate deployment actions

env:
  ALLURE_VERSION: 2.29.0
  RESULTS_DIR: allure-results
  SITE_DIR: allure-report

jobs:
  test:
    name: Run Pytest Benchmarks (Linux)
    runs-on: ubuntu-latest # â¬…ï¸ Requirement 2: Run On Linux only
    
    # Define outputs to pass the test status to the 'report' job
    outputs:
      test_outcome: ${{ steps.run_tests.outputs.test_outcome }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create Allure Results Directory
        run: mkdir -p ${{ env.RESULTS_DIR }}

      - name: Create Allure Support Files
        # â¬…ï¸ Requirement 6: environments.properties, categories.json, executor.json
        run: |
          # 1. environments.properties
          cp supports/environments.properties ${{ env.RESULTS_DIR }}/environments.properties
          
          # 2. categories.json (using a placeholder if no custom categories are defined)
          echo '[]' > ${{ env.RESULTS_DIR }}/categories.json
          
          # 3. executor.json (provides build info for the Allure dashboard)
          echo "{\"name\":\"GitHub Actions\",\"type\":\"GitHub\",\"url\":\"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\",\"buildName\":\"#${{ github.run_number }}.${{ github.run_attempt }}\",\"buildUrl\":\"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"}" > ${{ env.RESULTS_DIR }}/executor.json
      
      - name: Run Pytest Benchmarks
        id: run_tests
        run: |
          # â¬…ï¸ Requirement 2: Pytest command & Requirement 3: PASS/FAIL/UNSTABLE policy
          # Execute tests, capturing the exit code
          python -m pytest -v -m gpu --alluredir="${{ env.RESULTS_DIR }}"
          PYTEST_EXIT_CODE=$?
          
          # Determine test outcome based on exit code
          if [ $PYTEST_EXIT_CODE -eq 0 ]; then
            # Exit code 0: PASS - All tests passed
            OUTCOME="PASS"
          elif [ $PYTEST_EXIT_CODE -eq 1 ]; then
            # Exit code 1: UNSTABLE - At least one test failed
            OUTCOME="UNSTABLE"
          else
            # Exit code > 1: FAIL - Setup, config, or internal error
            OUTCOME="FAIL"
          fi
          
          echo "Test exit code: $PYTEST_EXIT_CODE. Outcome: $OUTCOME"
          echo "test_outcome=$OUTCOME" >> "$GITHUB_OUTPUT"
          
          # Ensure step succeeds even if tests are UNSTABLE (exit code 1) to allow artifact upload
        shell: bash
        continue-on-error: true 

      - name: Upload Raw Allure Results
        uses: actions/upload-artifact@v4
        with:
          name: allure-raw-results
          path: ${{ env.RESULTS_DIR }}

  report:
    name: Generate & Deploy Allure Report
    needs: test # â¬…ï¸ Requirement 5: Two jobs (test and report)
    runs-on: ubuntu-latest
    
    # Configure deployment environment
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Download Raw Allure Results
        uses: actions/download-artifact@v4
        with:
          name: allure-raw-results
          path: ${{ env.RESULTS_DIR }}
          
      - name: Download Previous Allure History
        uses: actions/download-artifact@v4
        with:
          name: allure-history
          path: previous_history # Download to a separate, temporary directory
        continue-on-error: true # â¬…ï¸ Allow this step to fail if the artifact is missing or expired    
      
      - name: Install Allure CLI & Generate Report
        run: |
          # Install Allure CLI
          wget https://repo.maven.apache.org/maven2/io/qameta/allure/allure-commandline/${{ env.ALLURE_VERSION }}/allure-commandline-${{ env.ALLURE_VERSION }}.zip -O allure.zip
          unzip allure.zip
          ALLURE_BIN=allure-commandline-${{ env.ALLURE_VERSION }}/bin/allure

          # â¬…ï¸ Requirement 4 & 7: Copy history for trends/dashboard
          # Check if history exists and move the 'history' directory into the new results
          if [ -d "previous_history/history" ]; then
            mkdir -p ${{ env.RESULTS_DIR }}/history
            mv previous_history/history/* ${{ env.RESULTS_DIR }}/history/ || true
            echo "Copied previous history for trends."
          fi
          
          # Generate report
          $ALLURE_BIN generate ${{ env.RESULTS_DIR }} --clean -o ${{ env.SITE_DIR }}

      - name: Upload Allure History for Next Run
        uses: actions/upload-artifact@v4
        with:
          name: allure-history
          path: ${{ env.SITE_DIR }}/history
          retention-days: 7
          
      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages Artifact
        # â¬…ï¸ Requirement 1: Build artifact for GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: ${{ env.SITE_DIR }}

      - name: Deploy to GitHub Pages
        id: deployment
        # â¬…ï¸ Requirement 1: Deploy with GitHub Actions
        uses: actions/deploy-pages@v4

      - name: Add Workflow Summary (Status Badges)
        run: |
          # Displays the final outcome based on the 'test' job result
          TEST_STATUS=${{ needs.test.outputs.test_outcome }}
          
          if [[ "$TEST_STATUS" == "FAIL" ]]; then
            STATUS_ICON="âŒ"
            STATUS_TEXT="Fail (Setup/Config Error)"
          elif [[ "$TEST_STATUS" == "UNSTABLE" ]]; then
            STATUS_ICON="âš ï¸"
            STATUS_TEXT="Unstable (Test(s) Failed)"
          else
            STATUS_ICON="âœ…"
            STATUS_TEXT="Pass (All Tests Succeeded)"
          fi

          echo "### ðŸ¤– GPU Benchmark CI/CD" >> $GITHUB_STEP_SUMMARY
          echo "Status: **$STATUS_ICON $STATUS_TEXT**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "[View Report](${{ steps.deployment.outputs.page_url }}/index.html)" >> $GITHUB_STEP_SUMMARY
        shell: bash