# ü§ñ GPU/CPU Benchmark Suite

**A Universal Benchmarking Framework for PyTorch-2, Tensorflow-2 Performance.**

Evaluate and compare GPU and CPU performance with unparalleled accuracy using PyTorch-2.9, Tensorflow-2.20, pytest, and detailed Allure reporting. This robust framework offers 
out-of-the-box support for heterogeneous hardware, including NVIDIA, AMD, Intel, DirectML, and standard CPU-only execution. Generate clear performance metrics 
and interactive dashboards to quickly identify bottlenecks and optimize model execution across any accelerator.

---

![PyTorch](https://img.shields.io/badge/PyTorch-2.9%2B-red.svg?style=for-the-badge&logo=PyTorch-2)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.20%2B-yellow.svg?style=for-the-badge&logo=TensorFlow-2)
![Pytest](https://img.shields.io/badge/pytest-8.4%2B-green.svg?style=for-the-badge&logo=pytest)
![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg?style=for-the-badge&logo=python3)
![Kubernetes](https://img.shields.io/badge/kubernetes-ready-yellow?style=for-the-badge&logo=kubernetes)
![Docker](https://img.shields.io/badge/docker-ready-blue?style=for-the-badge&logo=docker)
![CI/CD](https://img.shields.io/badge/CI/CD-ready-green?style=for-the-badge&logo=cicd)
![Allure Report](https://img.shields.io/badge/report-Allure-orange?style=for-the-badge&logo=allure)

---

## üõ†Ô∏è End-to-End DevOps & Kubernetes Workflow

This project is built on a comprehensive CI/CD pipeline and an automated Kubernetes deployment workflow:

### 1. Development & Testing (Local/CI)
* **Testing**: Developers run benchmarks locally using **Pytest** with specific markers (`-m gpu`, `-m cpu`) to validate performance and collect detailed results.
* **CI/CD (GitHub Actions / Jenkins)**: The **`ci.yml`** workflow in GitHub Actions (or an equivalent Jenkins pipeline) is triggered upon code changes.
    * It executes the **benchmark tests** against various hardware configurations.
    * It uses **Docker** to ensure a consistent, reproducible environment for testing.
    * It generates **Allure Reports** and plots system metrics (`scripts/plot_gpu_metrics.py`).

### 2. Packaging & Publishing
* **Docker Image Creation**: Using one of the provided `Dockerfile` variants (`Dockerfile.mini`, `Dockerfile.report`), a Docker image containing the test environment, report server, and dependencies is built.
* **Registry Push**: The final image is tagged and pushed to **Docker Hub** (or a private registry).

### 3. Automated Kubernetes Deployment
The **`deploy_gpu_workflow.py`** script manages the final deployment to a Kubernetes cluster:
* **Cluster Cleanup**: It first runs `kubectl delete deployment --all` for a clean state.
* **Dynamic GPU Detection**: It scans cluster nodes for available extended GPU resources (e.g., `gpu.intel.com/i915`, `nvidia.com/gpu`).
* **Resource Allocation**: The deployment manifest is dynamically configured to request the detected **GPU resource** or fall back to standard **CPU limits (1 core / 1Gi)**.
* **Deployment & Access**: It creates the optimized Kubernetes Deployment and Service. Once the Pod is running, it initiates a blocking **`kubectl port-forward`** to map the cluster service (Port 80) to your local machine (Port 8080), allowing instant, interactive access to the Allure Report dashboard via `http://127.0.0.1:8080`.

---

## üí° Project Overview  
This framework implements **GPU/CPU performance benchmarking** using `PyTorch`, `TensorFlow`, `Pytest`, `Pytest-benchmark`, and leveraging advanced CI/CD with Kubernetes & Docker.
It automatically detects available accelerators, measures inference throughput, GPU/CPU utilization, I/O, memory usage, etc., and produces **interactive Allure reports** for analysis.


| **Component** | **Technology** | **Role** |
|---------------|----------------|----------|
| Test Runner | **pytest** | Executes benchmark and stress tests. |
| Performance Metrics | **pytest-benchmark / SystemMetrics** | Measures FPS, CPU/GPU utilization, memory usage. |
| GPU Detection | **gpu_check.py** | Detects NVIDIA CUDA, AMD ROCm, Intel GPU, DirectML, or CPU fallback. |
| Reporting | **Allure** | Generates professional, interactive HTML dashboards with charts. |

---

## üöÄ Getting Started  

### üîß Prerequisites  
- üêç Python 3.10+ (recommended)  
- üìà Optional: Allure command-line tool for report viewing  
- üíª Windows or Linux system with GPU support (optional for CPU-only fallback)  

### ‚öôÔ∏è Installation  

Clone the repository:  
```bash
git clone https://github.com/luckyjoy/gpu_benchmark.git
cd gpu_benchmark
```

Run the setup script to create a virtual environment, install dependencies, and detect GPU on localhost.  
```bash

Usage:   python gpu_benchmark.py <Build_Number> [suite]
Example: python gpu_benchmark.py 4 gpu 
-> Run GPU Benchmark with GPU suite, build number 4, on localhost, and generate ALlure report. (No CI/CD, Kubernetes, Docker)

Usage:   python run_kubernestes.py <Build_Number> [Suite_marker or tests] [Dockerfile]
Example: python run_kubernestes.py 1 tests/test_data_preprocessing.py Dockerfile.custom
Example: python run_kubernestes.py 1 -m gpu Dockerfile.custom
Defaults: Dockerfile=Dockerfile.mini, Test=tests/test_data_preprocessing.py
-> Builds Docker images, runs GPU Benchmark [Suite_marker or tests] with build number 1, 
-> Generates Allure report, and pushes Docker images to Docker Hub.

Usage:   python deploy_gpu_workflow.py <Build_Number>
Example: python deploy_gpu_workflow.py 1
-> Creates the necessary Pod deployment and service, monitors Pod creation & reports scheduling events.
-> Deploys Docker image (build tag number 1) from Docker Hub, assigns a worker to run the Docker image within the assigned Pod.
-> Generates Allure Report.

```

The script will:  
- Create `venv310` if missing  
- Detect available GPU or fall back to CPU  
- Install all required packages in requirements.txt
- Run benchmark tests and store results in `allure-results/`  

---

## üê≥ Dockerized Execution (Optional)

Ensure consistent results across systems by running inside Docker.

### üß± Docker Image  
Image: **`gpu-benchmark:latest`** ‚Äî includes:  
- Python 3.10 or 3.11 environment  
- Required preinstalled packages
- Allure CLI for reporting  
- `/app` as working directory  

### ‚ñ∂Ô∏è Run Tests via Script  

**Script:** `run_docker.bat` (Windows)  
**Workflow:**  

| **Step** | **Description** |
|-----------|-----------------|
| 1Ô∏è‚É£ Check Docker | Verifies Docker Desktop is running. |
| 2Ô∏è‚É£ Clean Up | Deletes previous `allure-results` and `.benchmarks`. |
| 3Ô∏è‚É£ Build / Pull | Builds or updates Docker image. |
| 4Ô∏è‚É£ Execute Tests | Runs GPU/CPU benchmark suite. |
| 5Ô∏è‚É£ Generate Report | Produces Allure HTML output. |
| 6Ô∏è‚É£ Serve Report | Opens Allure dashboard locally. |

Command to execute:  
```bash
run_docker.bat
```

---

## üå≥ Framework Architecture  

```
gpu_benchmark/
‚îú‚îÄ Dockerfile                          # Main Docker build file
‚îú‚îÄ Dockerfile.mini                     # Minimal Docker build file
‚îú‚îÄ Dockerfile.report                   # Docker build file for the report server
‚îú‚îÄ Jenkinsfile						   # Option for Jenkins CI pipelines
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ pytest.ini                          # Pytest configuration
‚îú‚îÄ g.bat                               # Convenience batch file
‚îú‚îÄ gpu_benchmark.py                    # Main setup & execution script
‚îú‚îÄ deploy_gpu_workflow.py              # Kubernetes GPU auto-detection and deployment script
‚îú‚îÄ run_docker.py                       # Script to run tests inside Docker
‚îú‚îÄ run_gpu_benchmark.bat               # Windows batch script to run benchmarks
‚îú‚îÄ run_kubernestes.py                  # Kubernetes execution wrapper
‚îú‚îÄ gpu-workflow.yaml                   # Kubernetes manifest for deployment
‚îú‚îÄ gpu-benchmark-cpu-deployment.yaml   # Kubernetes manifest for CPU-only deployment
‚îú‚îÄ __init__.py
‚îú‚îÄ allure-report/                      # Static Allure HTML output directory
‚îú‚îÄ allure-results/                     # Pytest-Allure raw results directory
‚îú‚îÄ images/                             # Documentation image assets
‚îÇ  ‚îú‚îÄ allure_report.jpg
‚îÇ  ‚îî‚îÄ gpu_cpu_utilization.png
‚îú‚îÄ scripts/                            # Utility scripts for metrics, plotting, and trend analysis
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ gpu_utils.py
‚îÇ  ‚îú‚îÄ plot_gpu_metrics.py               # Generate charts for Allure
‚îÇ  ‚îú‚îÄ system_metrics.py                 # Capture CPU/GPU system metrics
‚îÇ  ‚îî‚îÄ update_trend.py
‚îú‚îÄ supports/                           # GPU detection and telemetry logic
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ categories.json
‚îÇ  ‚îú‚îÄ environments.properties
‚îÇ  ‚îú‚îÄ executor.json
‚îÇ  ‚îú‚îÄ gpu_check.py                      # Detects available hardware devices
‚îÇ  ‚îú‚îÄ gpu_monitor.py                    # Real-time GPU monitoring
‚îÇ  ‚îú‚îÄ performance_trend.py
‚îÇ  ‚îú‚îÄ telemetry_collector.py            # Gathers performance data
‚îÇ  ‚îú‚îÄ telemetry_hook.py
‚îÇ  ‚îú‚îÄ telemetry_trend.py
‚îÇ  ‚îú‚îÄ telemetry_visualizer.py
‚îÇ  ‚îú‚îÄ ubuntu.properties
‚îÇ  ‚îî‚îÄ windows.properties
‚îú‚îÄ tests/                              # Benchmark test cases
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ conftest.py                       # Pytest fixtures and hooks
‚îÇ  ‚îú‚îÄ device_utils.py                   # Utilities for device handling
‚îÇ  ‚îú‚îÄ test_amd_gpu_accelerator.py
‚îÇ  ‚îú‚îÄ test_cpu_reference.py             # CPU-only benchmarks
‚îÇ  ‚îú‚îÄ test_data_preprocessing.py
‚îÇ  ‚îú‚îÄ test_directml_gpu_accelerator.py
‚îÇ  ‚îú‚îÄ test_gpu_compute.py
‚îÇ  ‚îú‚îÄ test_gpu_convnet.py
‚îÇ  ‚îú‚îÄ test_gpu_matrix_mul.py
‚îÇ  ‚îú‚îÄ test_gpu_memory.py
‚îÇ  ‚îú‚îÄ test_gpu_mixed_precision.py
‚îÇ  ‚îú‚îÄ test_gpu_model_inference.py
‚îÇ  ‚îú‚îÄ test_gpu_stress.py
‚îÇ  ‚îú‚îÄ test_gpu_tensorflow_benchmark.py
‚îÇ  ‚îú‚îÄ test_gpu_transformer.py
‚îÇ  ‚îú‚îÄ test_idle_baseline.py
‚îÇ  ‚îú‚îÄ test_inference_load.py
‚îÇ  ‚îú‚îÄ test_intel_gpu_accelerator.py
‚îÇ  ‚îú‚îÄ test_io_accelerator.py
‚îÇ  ‚îú‚îÄ test_multi_gpu.py
‚îÇ  ‚îú‚îÄ test_network_io_accelerator.py
‚îÇ  ‚îú‚îÄ test_nvidia_gpu_accelerator.py
‚îÇ  ‚îú‚îÄ test_nvidia_real_gpu.py
‚îÇ  ‚îú‚îÄ test_nvidia_tensorrt_cudnn.py
‚îÇ  ‚îî‚îÄ test_parallel_training.py
‚îú‚îÄ .github/                            # GitHub Actions CI/CD workflows
‚îú‚îÄ venv310/                            # Virtual environment (auto-created)
‚îî‚îÄ .benchmarks/                        # Pytest-benchmark history
```

---

## üè∑Ô∏è Test Tags & Execution   

The framework uses **Pytest Markers (`-m`)** to categorize and select specific test suites for execution.

| **Tag** | **Focus Area** | **Description** |
|:--------|:----------------|:----------------|
| `gpu` | Core Benchmark | Tests running on any available accelerator (CUDA / ROCm / DirectML / Intel GPU). |
| `cpu` | Fallback / Reference | Tests running on CPU fallback. |
| `nvidia` | NVIDIA-Specific | Tests targeting NVIDIA CUDA features (e.g., CUDA, Tensor Cores). |
| `amd` | AMD-Specific | Tests targeting AMD ROCm features. |
| `intel` | Intel-Specific | Tests targeting Intel oneAPI / i915 features. |
| `directml` | DirectML-Specific | Tests targeting DirectML features (Windows / WSL). |
| `benchmark` | Performance Metric | Measures FPS, utilization, memory, and throughput. |
| `stress` | Endurance / Load | Heavy-load GPU endurance tests. |

---

### üß™ Local Command Line Execution  

Use the commands below to execute specific test suites and generate Allure data locally.

| **Execution Mode** | **Command** |
|:--------------------|:------------|
| Run All GPU Benchmarks | `pytest -m gpu --alluredir=allure-results -v` |
| Run All CPU Benchmarks | `pytest -m cpu --alluredir=allure-results -v` |
| Run Specific Tag (e.g., Performance + GPU) | `pytest -m "benchmark and gpu" --alluredir=allure-results` |
| Run GPU/CPU Combined | `pytest -m "benchmark or gpu or cpu"` |

---

## üìä Professional Test Reporting  

### 1Ô∏è‚É£**Interactive Allure Report (Recommended)**  
```bash
python -m venv venv310
Linux/macOS (Bash/Zsh):	source venv310/bin/activate
Windows (Command Prompt): call venv310\Scripts\activate
pytest --alluredir=allure-results
pytest /tests/test_gpu_tensorflow_benchmark --alluredir=allure-results
pytest -m "gpu or cpu" --alluredir=allure-results
pytest -m gpu --alluredir=allure-results
allure serve allure-results

```
üì∏ *Allure Report Preview:* 

![Build History Dashboard](images/dashboard.jpg)

![Allure Overview Report](images/allure_report.jpg)

![GPU Utilization](images/gpu_cpu_utilization.png)


> Opens an interactive HTML dashboard with detailed execution insights.

### 2Ô∏è‚É£ **Static HTML Report (Optional)**  
```bash
pytest --html=reports/report.html --self-contained-html
```

---

## ‚öôÔ∏è CI/CD Integration

| System | Description |
|--------|-------------|
| Jenkins / GitHub Actions | Automates test execution and report generation |
| Docker | Guarantees repeatable benchmark environments |
| Allure | Produces professional dashboards for CI/CD pipelines |

---

## ü§ù Contributing Guidelines

1. Fork the repository  
2. Create a feature branch  
3. Implement new tests, benchmarks, or reporting features  
4. Run `pytest -v` locally and verify results  
5. Submit a Pull Request with a clear description  

**Code Style:**  
- Follow **PEP8** conventions  
- Use **pytest markers** consistently  
- Ensure **Allure reports** generate without errors  
- Document new metrics or tests in **Allure charts**  

---

## ü™™ License

Released under the **MIT License** ‚Äî free to use, modify, and distribute.

---

üì¨ *Contact:* Bang Thien Nguyen [ontario1998@gmail.com](mailto:ontario1998@gmail.com)  

---

> _‚ÄúMeasure performance before you optimize ‚Äî know your hardware before you test your code.‚Äù_
